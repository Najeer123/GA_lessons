{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Web APIs & Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given posts from two Using Reddit's API, you'll collect posts from two subreddits, r/worldnews and r/todayilearned, we will use NLP to train a classifier on which subreddit a given post came from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary\n",
    "\n",
    "### Contents:\n",
    "- [Scraping reddit for data](#Scraping-reddit-for-data)\n",
    "- [2018 Data Import and Cleaning](#2018-Data-Import-and-Cleaning)\n",
    "- [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n",
    "- [Data Visualization](#Visualize-the-data)\n",
    "- [Descriptive and Inferential Statistics](#Descriptive-and-Inferential-Statistics)\n",
    "- [Outside Research](#Outside-Research)\n",
    "- [Conclusions and Recommendations](#Conclusions-and-Recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping reddit for data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Scrape reddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.reddit.com/r/worldnews.json'\n",
    "url2 = 'https://www.reddit.com/r/todayilearned.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subreddit_scraper(subreddit, n_posts):\n",
    "    '''\n",
    "    Returns DataFrame of subreddit posts.\n",
    "    '''\n",
    "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
    "    headers = {'User-agent': 'srs 0.01'}\n",
    "    posts = []\n",
    "    after = None\n",
    "    # Reddit returns 25 posts per scrape.\n",
    "    n_scrapes = int(np.ceil(n_posts/25))  \n",
    "    for i in range(n_scrapes):\n",
    "        \n",
    "        # Load indicator.\n",
    "        if i%5 == 0:\n",
    "            print(f'scrape {i} in progress...')\n",
    "\n",
    "        # Use original url for first scrape.\n",
    "        if after is None:\n",
    "            params = {}\n",
    "        else:\n",
    "            params = {'after': after}\n",
    "\n",
    "        r = requests.get(url, params=params, headers=headers)\n",
    "        if r.status_code == 200:\n",
    "            js = r.json()\n",
    "            posts.extend(js['data']['children'])\n",
    "            after = js['data']['after']\n",
    "        else:\n",
    "            print(r.status_code)\n",
    "            break\n",
    "\n",
    "        # Randomize sleep timing.\n",
    "        time.sleep(random.random()*7)\n",
    "    \n",
    "    # Remove repeats.\n",
    "    names = []\n",
    "    posts_nr = []\n",
    "    for p in posts:\n",
    "        if p['data']['name'] not in names:\n",
    "            names.append(p['data']['name'])\n",
    "            posts_nr.append(p)\n",
    "    \n",
    "    df_interm = pd.DataFrame(posts_nr)\n",
    "    # Probably the easiest way to unpack a list of dictionaries.\n",
    "    df = pd.DataFrame(df_interm['data'].apply(pd.Series))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scrape 0 in progress...\n",
      "scrape 5 in progress...\n",
      "scrape 10 in progress...\n",
      "scrape 15 in progress...\n",
      "scrape 20 in progress...\n",
      "scrape 25 in progress...\n",
      "scrape 30 in progress...\n",
      "scrape 35 in progress...\n"
     ]
    }
   ],
   "source": [
    "df_wn = subreddit_scraper('worldnews', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(795, 104)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Save dataset after scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_path = r'..\\datasets\\worldnews.csv'\n",
    "df_wn.to_csv(export_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scrape 0 in progress...\n",
      "scrape 5 in progress...\n",
      "scrape 10 in progress...\n",
      "scrape 15 in progress...\n",
      "scrape 20 in progress...\n",
      "scrape 25 in progress...\n",
      "scrape 30 in progress...\n",
      "scrape 35 in progress...\n"
     ]
    }
   ],
   "source": [
    "df_til = subreddit_scraper('todayilearned', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(674, 105)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_til.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_path = r'..\\datasets\\todayilearned.csv'\n",
    "df_wn.to_csv(export_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
